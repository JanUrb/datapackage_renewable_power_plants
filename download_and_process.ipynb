{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Power System Data: Renewable Energy Power Plant List\n",
    "\n",
    "\n",
    "## Part 1: Download and Processing the Original Data\n",
    "\n",
    "This script downlads and extracts the original data from the sources and merges them in a single data\n",
    "frame. It subsequently adds the geolocation for each power plant and gives an overview over the\n",
    "structure of the data frame. Finally it saves the data frame to a SQLite file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents \n",
    "\n",
    "* [1. Script Setup](#1.-Script-Setup)\n",
    "* [2. Download and Data Extraction](#2.-Download-and-Data-Extraction)\n",
    "    * [2.1 Download and Create Individual Data Frames](##-2.1-Download-and-Create-Individual-Data-Frames)\n",
    "    * [2.2 Definition of Data Frame Structure](##-2.2-Definition-of-Data-Frame-Structure)\n",
    "    * [2.3 Merge Data Frames](##2.3-Merge-Data-Frames)\n",
    "    * [2.4 Translation and Summary](##2.4-Translation-and-Summary)\n",
    "* [3 Georeferencing](#3-Georeferencing)\n",
    "    * [3.1 Get Coordinates by Postcode](##-3.1-Get-Coordinates-by-Postcode)\n",
    "    * [3.2 Transform Geoinformation](##3.2-Transform-Geoinformation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Script Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing all necessary Python libraries for this Script\n",
    "#%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import posixpath\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# not in use\n",
    "# import requests\n",
    "import datetime  \n",
    "import sqlite3 \n",
    "import utm\n",
    "import logging\n",
    "\n",
    "# Set up a log\n",
    "logger = logging.getLogger('notebook')\n",
    "logger.setLevel('INFO')\n",
    "nb_root_logger = logging.getLogger()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s'\\\n",
    "                              '- %(message)s',datefmt='%d %b %Y %H:%M:%S')\n",
    "nb_root_logger.handlers[0].setFormatter(formatter)\n",
    "\n",
    "# Create input and output folders if they don't exist\n",
    "os.makedirs('input/original_data', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('output/datapackage_renewables', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data which will be processed below is provided by the following sources:\n",
    "\n",
    "**1. [Netztransparenz.de](https://www.netztransparenz.de/de/Anlagenstammdaten.htm)** - Official grid transparency platform from the German TSOs (50Hertz, Amprion, TenneT and TransnetBW).\n",
    "\n",
    "**2. Bundesnetzagentur (BNetzA)** - German Federal Network Agency for Electricity, Gas, Telecommunications, Posts and Railway (Data for [roof-mounted PV power plants](http://www.bundesnetzagentur.de/cln_1422/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/ErneuerbareEnergien/Photovoltaik/DatenMeldgn_EEG-VergSaetze/DatenMeldgn_EEG-VergSaetze_node.html) and for [all other renewable energy power plants](http://www.bundesnetzagentur.de/cln_1412/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/ErneuerbareEnergien/Anlagenregister/Anlagenregister_Veroeffentlichung/Anlagenregister_Veroeffentlichungen_node.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Download and Create Individual Data Frames\n",
    "\n",
    "**Specify the source URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# point URLs to original data\n",
    "url_netztransparenz ='https://www.netztransparenz.de/de/file/'\\\n",
    "                  'Anlagenstammdaten_2014_4UeNB.zip'\n",
    "\n",
    "url_bnetza ='http://www.bundesnetzagentur.de/SharedDocs/Downloads/DE/Sachgebiete/'\\\n",
    "            'Energie/Unternehmen_Institutionen/ErneuerbareEnergien/Anlagenregister/'\\\n",
    "            'VOeFF_Anlagenregister/2016_04_Veroeff_AnlReg.xls?__blob=publicationFile&v=2'\n",
    "        \n",
    "url_bnetza_pv = 'https://www.bundesnetzagentur.de/SharedDocs/Downloads/DE/'\\\n",
    "                'Sachgebiete/Energie/Unternehmen_Institutionen/'\\\n",
    "                'ErneuerbareEnergien/Photovoltaik/Datenmeldungen/'\\\n",
    "                'Meldungen_Aug-Mrz2016.xls?__blob=publicationFile&v=2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_and_cache(url):\n",
    "    \"\"\"This function downloads a file into a folder called \n",
    "    original_data and returns the local filepath.\"\"\"\n",
    "    \n",
    "    path = urllib.parse.urlsplit(url).path\n",
    "    filename = posixpath.basename(path)\n",
    "    filepath = \"input/original_data/\"+filename\n",
    "    \n",
    "    #check if file exists, if not download it\n",
    "    if  not os.path.exists(filepath):\n",
    "        print(\"Downloading file\", filename)\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    else:\n",
    "        print(\"Using local file from\", filepath)\n",
    "    filepath = ''+filepath\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unzip and load downloaded data as data frames**\n",
    "\n",
    "To process the provided data [pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) is applied.<br>\n",
    "*This section of data loading takes approx. 5-15 min. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file Anlagenstammdaten_2014_4UeNB.zip\n",
      "Download and cache bnetz_a\n",
      "Downloading file 2016_04_Veroeff_AnlReg.xls\n",
      "Downloading file Meldungen_Aug-Mrz2016.xls\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1239\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m                 \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iso-8859-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1128\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body)\u001b[0m\n\u001b[0;32m   1078\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    853\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    855\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1236\u001b[0m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[1;32m-> 1237\u001b[1;33m                                                   server_hostname=server_hostname)\n\u001b[0m\u001b[0;32m   1238\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\u001b[0m\n\u001b[0;32m    375\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                          _context=self)\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context)\u001b[0m\n\u001b[0;32m    746\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    982\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;34m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f7fec412a167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# Get BNetzA-PV register\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mbnetza_pv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_and_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_bnetza_pv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Combine all PV BNetzA sheets into one data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6da45f6b97d5>\u001b[0m in \u001b[0;36mdownload_and_cache\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m  \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloading file\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using local file from\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 483\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1283\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KRa\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1240\u001b[0m                 \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)>"
     ]
    }
   ],
   "source": [
    "# load zip file for data from netztransparenz.de\n",
    "z = zipfile.ZipFile(download_and_cache(url_netztransparenz))\n",
    "\n",
    "\n",
    "# Get TSO data from zip file\n",
    "amprion_df = pd.read_csv(z.open('Amprion_Anlagenstammdaten_2014.csv'),\n",
    "                         sep=';',\n",
    "                         thousands='.',\n",
    "                         decimal=',',\n",
    "                         header=0,\n",
    "                         parse_dates=[11, 12, 13, 14],\n",
    "                         encoding='cp850',\n",
    "                         dayfirst=True,\n",
    "                         low_memory=False)\n",
    "\n",
    "hertz_df = pd.read_csv(z.open('50Hertz_Anlagenstammdaten_2014.csv'),\n",
    "                       sep=';',\n",
    "                       thousands='.',\n",
    "                       decimal=',',\n",
    "                       header=0,\n",
    "                       parse_dates=[11, 12, 13, 14],\n",
    "                       encoding='cp1252',\n",
    "                       dayfirst=True,\n",
    "                       low_memory=False)\n",
    "\n",
    "tennet_df = pd.read_csv(z.open('TenneT_Anlagenstammdaten_2014.csv'),\n",
    "                        sep=';',\n",
    "                        thousands='.',\n",
    "                        decimal=',',\n",
    "                        header=0,\n",
    "                        parse_dates=[11, 12, 13, 14],\n",
    "                        encoding='cp1252',\n",
    "                        dayfirst=True,\n",
    "                        low_memory=False)\n",
    "\n",
    "transnetbw_df = pd.read_csv(z.open('TransnetBW_Anlagenstammdaten_2014.csv'),\n",
    "                            sep=';',\n",
    "                            thousands='.',\n",
    "                            decimal=',',\n",
    "                            header=0,\n",
    "                            parse_dates=[11, 12, 13, 14],\n",
    "                            encoding='cp1252',\n",
    "                            dayfirst=True,\n",
    "                            low_memory=False)\n",
    "\n",
    "print('Download and cache bnetz_a')\n",
    "# Get BNetzA register \n",
    "bnetza_df = pd.read_excel(download_and_cache(url_bnetza),\n",
    "                          sheetname='Gesamtübersicht',\n",
    "                          header=0,\n",
    "                          converters={'4.9 Postleit-zahl': str})\n",
    "\n",
    "# Get BNetzA-PV register\n",
    "bnetza_pv = pd.ExcelFile(download_and_cache(url_bnetza_pv))\n",
    "\n",
    "\n",
    "# Combine all PV BNetzA sheets into one data frame\n",
    "bnetza_pv_df = pd.concat(bnetza_pv.parse(sheet, skiprows=10,\n",
    "                                         converters={'Anlage \\nPLZ': str}\n",
    "                                         ) for sheet in bnetza_pv.sheet_names)\n",
    "\n",
    "# Drop not needed NULL \"Unnamed:\" column\n",
    "bnetza_pv_df = bnetza_pv_df.drop(bnetza_pv_df.columns[[7]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Definition of Data Frame Structure\n",
    "\n",
    "**Translation**<br>\n",
    "To standardise the data frame the original column names from the German TSOs and the BNetzA wil be translated and new english column names wil be assigned to the data frame. The unique column names are needed in order to merge the data frames together.<br>\n",
    "The translation list is provided here: **[csv](https://github.com/Open-Power-System-Data/datapackage_renewable_power_plants/blob/master/input/column_translation_list.csv).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get translation list\n",
    "columnnames = pd.read_csv('input/column_translation_list.csv', sep=\",\",\n",
    "                          header=0)\n",
    "\n",
    "columndict = columnnames.set_index('original_name')['column_naming'].to_dict()\n",
    "\n",
    "# Translate columns by list \n",
    "bnetza_pv_df.rename(columns=columndict, inplace=True)\n",
    "bnetza_df.rename(columns=columndict, inplace=True)\n",
    "transnetbw_df.rename(columns=columndict, inplace=True)\n",
    "tennet_df.rename(columns=columndict, inplace=True)\n",
    "amprion_df.rename(columns=columndict, inplace=True)\n",
    "hertz_df.rename(columns=columndict, inplace=True)\n",
    "\n",
    "# Translate special cases separately\n",
    "backslash = {'Anlage \\nBundesland': 'federal_state', 'Anlage \\nOrt oder Gemarkung':\n",
    "             'city', 'Anlage \\nPLZ': 'postcode', 'Anlage \\nStraße oder Flurstück *)':\n",
    "             'address', 'Installierte \\nNennleistung [kWp]': 'electrical_capacity'}\n",
    "\n",
    "bnetza_pv_df.rename(columns = backslash, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add source names and generation types**<br>\n",
    "In a second step all source names and (for the BNetzA-PV data) the generation types will be added separately to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add source names to the data frames\n",
    "transnetbw_df['source'] = 'TransnetBW'\n",
    "tennet_df['source'] = 'TenneT'\n",
    "amprion_df['source'] = 'Amprion'\n",
    "hertz_df['source'] = '50Hertz'\n",
    "bnetza_df['source'] = 'BNetzA'\n",
    "bnetza_pv_df['source'] = 'BNetzA_PV'\n",
    "\n",
    "# Add for the BNetzA PV data the generation types\n",
    "bnetza_pv_df['generation_type'] = 'solar'\n",
    "bnetza_pv_df['generation_subtype'] = 'solar_roof_mounted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Merge Data Frames\n",
    "\n",
    "The individual data frames from the TSOs (Netztransparenz.de) and BNetzA will be merged together.<br>\n",
    "The column *generation_subtype* will be filled with values from the Column *generation_type* (it wil be used in the next step 2.4 to differentiate between generation types and subtypes).<br>\n",
    "In the last step of this section only the columns of interest will be selected and the final data frame created.\n",
    "\n",
    "**Merge individual data frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge data frames\n",
    "dataframes = [transnetbw_df, tennet_df, amprion_df, hertz_df, bnetza_pv_df, bnetza_df]\n",
    "\n",
    "renewables = pd.concat(dataframes)\n",
    "\n",
    "renewables = renewables.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill column \"generation_subtype\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transfer generation_type values to generation_subtype\n",
    "idx_subtype = renewables[(renewables['source'] != 'BNetzA_PV')].index\n",
    "\n",
    "renewables['generation_subtype'].loc[idx_subtype] = (renewables['generation_type'].loc[idx_subtype])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select columns of our interest**\n",
    "\n",
    "In this section only the columns of interest for the final data frame are selected and the rest discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_interest = ['start_up_date', 'electrical_capacity', 'generation_type',\n",
    "                   'generation_subtype', 'thermal_capacity', 'city', 'postcode',\n",
    "                   'address', 'tso', 'dso', 'utm_zone', 'utm_east', 'utm_north',\n",
    "                   'notification_reason', 'eeg_id',\n",
    "                   'voltage_level', 'decommission_date',\n",
    "                   'power_plant_id', 'source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renewables = renewables.loc[:, column_interest]\n",
    "renewables.reset_index(drop=True)\n",
    "logger.info('Clean dataframe from not needed columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First look at the final data frame structure and format**\n",
    "\n",
    "`DataFrame.info()` shows us the number of non-null (non-NA) values in each column, which can serve as a first indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renewables.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Translation and Summary\n",
    "\n",
    "In this section the different German terms for generation types, subtypes and voltage levels will be translated and harmonized across the individual data sources (see [value_translation_list.csv](https://github.com/Open-Power-System-Data/datapackage_renewable_power_plants/blob/master/input/value_translation_list.csv)).\n",
    "\n",
    "Additionally the different classifications and terms for generation types and subtypes from the individual sources will be assigned accordign to the following table (see [generation_types_translation_list.csv](https://github.com/Open-Power-System-Data/datapackage_renewable_power_plants/blob/master/input/generation_types_translation_list.csv)):\n",
    "\n",
    "|generation_type|generation_subtype|\n",
    "|---|---|\n",
    "|wind|wind_onshore|\n",
    "| |wind_offshore|\n",
    "|solar|solar_ground_mounted|\n",
    "||solar_roof_mounted|\n",
    "|gas|gas|\n",
    "||gas_sewage|\n",
    "||gas_landfill|\n",
    "||gas_mine|\n",
    "|biomass|biomass|\n",
    "||biofuel|\n",
    "||biogas|\n",
    "||biogas_from_grid|\n",
    "||biogas_dry_fermentation|\n",
    "||wood|\n",
    "||waste_wood|\n",
    "|hydro|hydro|\n",
    "|geothermal|geothermal|\n",
    "\n",
    "**Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read translation list\n",
    "translation_list = pd.read_csv('input/value_translation_list.csv', \n",
    "                               sep=\",\",\n",
    "                               header=0)\n",
    "# Create dictionnary in order to change values \n",
    "translation_dict = translation_list.set_index('original_name')['opsd_naming'].to_dict()\n",
    "\n",
    "postcode = pd.read_csv('input/de_tso_postcode_gps.csv',\n",
    "                       sep=';',\n",
    "                       header=0)\n",
    "\n",
    "types_dict = types_list.set_index('generation_subtype')['generation_type'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate and assign generation types and subtypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running time ~ 10 min\n",
    "renewables.replace(translation_dict, inplace=True)\n",
    "\n",
    "# Create new column for generation_subtype\n",
    "renewables['generation_subtype'] = renewables.generation_type\n",
    "\n",
    "# Replace generation_subtype by generation_type\n",
    "renewables.generation_type.replace(types_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Electrical capacity per generation type (in MW)\n",
    "renewables.groupby(['generation_type'])['electrical_capacity'].sum() / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Electrical capacity per generation subtype (in MW)\n",
    "renewables.groupby(['generation_subtype'])['electrical_capacity'].sum() / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Georeferencing\n",
    "## 3.1 Get Coordinates by Postcode\n",
    "*(for data with no existing geocoordinates)*\n",
    "\n",
    "The available post code in the original data provides a first approximation for the geocoordinates of the RE power plants.<br>\n",
    "The BNetzA data provides the full zip code whereas due to data privacy the TSOs only report the first three digits of the power plant's post code (e.g. 024xx) and no address. Subsequently a centroid of the post code region polygon is used to find the coordinates.\n",
    "\n",
    "With data from\n",
    "*  http://www.suche-postleitzahl.org/downloads?download=plz-gebiete.shp.zip\n",
    "*  http://www.suche-postleitzahl.org/downloads?download_file=plz-3stellig.shp.zip\n",
    "*  http://www.suche-postleitzahl.org/downloads\n",
    "\n",
    "a CSV-file for all existing German post codes with matching geocoordinates has been compiled. The latitude and longitude coordinates were generated by running a PostgreSQL + PostGIS database. Additionally the respective TSO has been added to each post code. *(A Link to the SQL script will follow here later)*\n",
    "\n",
    "*(License: http://www.suche-postleitzahl.org/downloads, Open Database Licence for free use. Source of data: © OpenStreetMap contributors)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read generated postcode/location file\n",
    "postcode = pd.read_csv('input/de_tso_postcode_gps.csv',\n",
    "                       sep=';',\n",
    "                       header=0)\n",
    "\n",
    "# Drop possible duplicates in postcodes\n",
    "postcode.drop_duplicates('postcode', keep='last',inplace=True)\n",
    "\n",
    "# Show first entries\n",
    "postcode.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "** Merge geometry information by using the postcode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take postcode and longitude/latitude infomations\n",
    "postcode= postcode[[0,3,4]]\n",
    "\n",
    "renewables =renewables.merge(postcode, on=['postcode'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transform Geoinformation\n",
    "*(for data with already existing geoinformation)*\n",
    "\n",
    "In this section the existing geoinformation (in UTM-format) will be transformed into latidude and longitude coordiates as a uniform standard for geoinformation. \n",
    "\n",
    "The BNetzA data set offers UTM Geoinformation with the columns *utm_zone (UTM-Zonenwert)*, *utm_east* and *utm_north*. Most of utm_east-values include the utm_zone-value **32** at the beginning of the number. In order to properly standardize and transform this geoinformation into latitude and longitude it is necessary to remove this utm_zone value. (For all UTM entries the utm_zone 32 is used by the BNetzA.)\n",
    "\n",
    "\n",
    "|utm_zone|\t utm_east|\t utm_north| comment|\n",
    "|---|---|---| ----|\n",
    "|32|\t413151.72|\t6027467.73| proper coordinates|\n",
    "|32|\t**32**912159.6008|\t5692423.9664| caused error by 32|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many different utm_zone values are in the data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renewables.groupby(['utm_zone'])['utm_zone'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the utm_zone \"32\" from the utm_east value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find entries with 32 value at the beginning\n",
    "ix_32 = (renewables['utm_east'].astype(str).str[:2]=='32')\n",
    "ix_notnull= renewables['utm_east'].notnull()\n",
    "\n",
    "# Remove 32 from utm_east entries\n",
    "renewables.loc[ix_32, 'utm_east'] = renewables.loc[ix_32, 'utm_east'].astype(str).str[2:].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversion UTM to lat/lon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert from UTM values to latitude and longitude coordinates\n",
    "try:\n",
    "    renewables['lonlat'] = renewables.loc[ix_notnull, ['utm_east', 'utm_north', 'utm_zone']].apply(\n",
    "        lambda x: utm.to_latlon(x[0], x[1], x[2], 'U'),\n",
    "        axis=1) \\\n",
    "        .astype(str)\n",
    "        \n",
    "except:\n",
    "    renewables['lonlat'] = np.NaN\n",
    "    \n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "for row in renewables['lonlat']:\n",
    "    try:\n",
    "        # Split tuple format into the column lat and lon  \n",
    "        row = row.lstrip('(').rstrip(')')\n",
    "        lat.append(row.split(',')[0])\n",
    "        lon.append(row.split(',')[1])\n",
    "    except:\n",
    "        # set NAN \n",
    "        lat.append(np.NaN)\n",
    "        lon.append(np.NaN)\n",
    "          \n",
    "\n",
    "renewables['latitude'] = lat\n",
    "renewables['longitude'] = lon    \n",
    "\n",
    "# Add new values to data frame lon and lat\n",
    "renewables['lon'] = renewables[['longitude', 'lon']].apply(\n",
    "    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n",
    "    axis=1)\n",
    "\n",
    "renewables['lat'] = renewables[['latitude', 'lat']].apply(\n",
    "    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check: missing coordinates by source and type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Missing Coordinates ',renewables.lat.isnull().sum())\n",
    "\n",
    "renewables[renewables.lat.isnull()].groupby(['generation_type',\n",
    "                                             'source']\n",
    "                                            )['source'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renewables.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Save Data Frame\n",
    " \n",
    " Finally the merged data frame will be saved as SQLite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "renewables.to_sql('raw_data_output', sqlite3.connect('raw_data.sqlite'), if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The check, validation of the renewable power plants list and the Creation of CSV/XLSX/SQLite files can be found in Part 2 of this script. It also generates a daily timeseries of cumulated installed capacities by generation types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
